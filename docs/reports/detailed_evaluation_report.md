# Open-Web-Search Transparent Evaluation Report

**Query**: DeepSeek-V3 architecture innovations (MLA, MoE) and training cost efficiency compared to Llama 3.1
**Generated At**: 2026-01-29 06:34:41.391635+00:00

## 1. Planning Layer (Query Decomposition)
The LLM decomposed the user query into the following search queries:
- `DeepSeek-V3 architecture innovations including MLA and MoE technical specifications and design principles`
- `Training cost efficiency analysis of DeepSeek-V3 compared to Llama 3.1 in terms of computational resources and energy consumption`
- `Comparative performance evaluation of DeepSeek-V3 and Llama 3.1 in real-world applications highlighting efficiency and scalability`

## 2. Retrieval Layer (Raw Search Results)
The Search Engine (SearXNG) returned the following URLs:
### 1. [DeepSeek-V3TechnicalReport](https://arxiv.org/pdf/2412.19437)
- **Snippet**: A Liu 저술 · 2024 · 3364회 인용 — We presentDeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.
- **Engine**: searxng
### 2. [DeepSeek-V3TechnicalReport](https://arxiv.org/html/2412.19437v1)
- **Snippet**: We presentDeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.
- **Engine**: searxng
### 3. [UnpackingDeepSeek-V3: From Architectural Renovations to ...](https://www.techrxiv.org/users/962529/articles/1331367-unpacking-deepseek-v3-from-architectural-renovations-to-technical-innovations)
- **Snippet**: D Zhang 저술 — This paper presents an anatomical overview ofDeepSeek-V3architecture... UnpackingDeepSeek-V3: From Architectural Renovations toTechnical...
- **Engine**: searxng
### 4. [DeepSeek model V3와 R1의 모든 것](https://sonovman.blogspot.com/2025/02/deepseek-model-v3-r1.html)
- **Snippet**: 2025. 2. 2. — 아키텍처(Architecture). 1) Transformer with MHLAandMoE.DeepSeek-V3모델은 Transformer 프레임워크를 사용합니다. 전통적인 Transformer(Vaswani ...
- **Engine**: searxng
### 5. [Insights intoDeepSeek-V3: Scaling ChallengesandReflections on ...](https://dl.acm.org/doi/10.1145/3695053.3731412)
- **Snippet**: 2025. 6. 20. — This paper presents an in-depth analysis of theDeepSeek-V3/R1 modelarchitectureandits AI infrastructure, highlighting keyinnovationssuch ...
- **Engine**: searxng
### 6. [DeepSeek-V3: A High-Performance Mixture-of-Experts Language Model](https://www.researchgate.net/publication/388484212_DeepSeek-V3_A_High-Performance_Mixture-of-Experts_Language_Model)
- **Snippet**: 2025. 1. 29. —DeepSeek-V3is a cutting-edge Mixture-of-Experts (MoE) language model featuring 671 billion total parameters, with 37 billion activated per token.
- **Engine**: searxng
### 7. [DeepSeekandthe End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era)
- **Snippet**: 2025. 1. 31. — When combined with theirMLAarchitectureandauxiliary-loss-freeMoE, DeepSeek reports: 2.5x speedup in text generation. 40% reduction in ...
- **Engine**: searxng
### 8. [DeepSeek-V3New Paper is coming! Unveiling the Secrets of Low-Cost ...](https://synced.medium.com/deepseek-v3-new-paper-is-coming-97b2761fd3e6)
- **Snippet**: This research focuses on the interplay between hardwarearchitectureandmodeldesignin achieving economical large-scale trainingandinference.
- **Engine**: searxng
### 9. [DeepSeek-V3TechnicalReport - JunHan's AI Factory - 티스토리](https://junhan-ai.tistory.com/509)
- **Snippet**: 2025. 2. 14. — 우리는 토큰당 37B가 활성화되고 총 671B 파라미터를 갖춘 강력한 Mixture-of-Experts(MoE) 언어 모델인 DeepSeek-V3를 소개한다. 효율적인 추론과 비용 ...
- **Engine**: searxng
### 10. [DeepSeekArchitectureandThe Aha Moment](https://www.infosys.com/iki/techcompass/deepseek-architecture-aha-moment.html)
- **Snippet**: This white paper examines the architecturalinnovations, engineering advancements,andpractical implications of DeepSeek models for enterprise adoption.
- **Engine**: searxng
### 11. [DeepSeek V3TrainingCost: Here's How It ComparesToLlama3.1(405B)](https://apxml.com/posts/training-cost-deepseek-v3-vs-llama-3)
- **Snippet**: 2025. 1. 26. — Depending on final GPU utilization, overhead, thetrainingcosts can lie between $92.4 millionand$123.2 million, an orderofmagnitude higher ...
- **Engine**: searxng
### 12. [DeepSeek V3andthe actualcostoftrainingfrontier AI models](https://www.interconnects.ai/p/deepseek-v3-and-the-actual-cost-of)
- **Snippet**: 2025. 1. 9. — This post revisits the technical detailsofDeepSeek V3, but focuses on how besttoview thecostoftrainingmodels at the frontierofAIandhow these costs ...
- **Engine**: searxng
### 13. [Insights intoDeepSeek-V3: Scaling ChallengesandReflections on ...](https://medium.com/@EleventhHourEnthusiast/insights-into-deepseek-v3-scaling-challenges-and-reflections-on-hardware-for-ai-architectures-2f108a8aeb28)
- **Snippet**: DeepSeek-V3demonstrates how careful engineering can achieve high-performance AI with significantly fewerresourcesthan traditional approaches.
- **Engine**: searxng
### 14. [DeepSeek-V3, ultra-large open-source AI, outperformsLlamaand...](https://www.facebook.com/groups/chiefai/posts/2591340861059200/)
- **Snippet**: DeepSeek-V3's selective activation means lower powerconsumptionandmore efficient useofcomputingresources. The model'sefficiency...
- **Engine**: searxng
### 15. [L3D-RAG: LeveragingLLaMA3.1andDeepSeek with Retrieval- ...](https://www.sciencedirect.com/science/article/pii/S1110016825010336)
- **Snippet**: SE Sorour 저술 · 2025 · 1회 인용 — This study introduces L3D-RAG, a generative framework designedtoevaluate the performanceoftwo generative AI models:LLaMA3.1andDeepSeek.
- **Engine**: searxng
### 16. [Notes on the new Deepseek v3](https://composio.dev/blog/notes-on-new-deepseek-v3)
- **Snippet**: 2025. 1. 1. — Currently, it is the best open-source model, beatingLlama3.1405b, Qwen,andMistral. It is on par with OpenAI GPT-4oandClaude 3.5 Sonnet ...
- **Engine**: searxng
### 17. [DeepSeek model impact on AI hardware companies](https://discussion.fool.com/t/deepseek-model-impact-on-ai-hardware-companies/112703)
- **Snippet**: 2025. 1. 27. — DeepSeek first caught our attention after a CNBC report revealed that its DeepSeek V3 model had outperformed Meta'sLlama3.1, OpenAI's GPT-4o, ...
- **Engine**: searxng
### 18. [A Technical ReviewofDeepSeek AI: CapabilitiesandComparisons with ...](https://www.preprints.org/frontend/manuscript/f0c0f77aada81c46354fb6638a0bc0e3/download_pub)
- **Snippet**: S Joshi 저술 · 2025 · 1회 인용 —EnergyEfficiency: Consumes 40% lessenergyper inference than comparable models [26]. •.TrainingCost: Achieved comparable performance at $6M ...
- **Engine**: searxng
### 19. [Insights intoDeepSeek-V3: Scaling ChallengesandReflections on ...](https://arxiv.org/html/2505.09343v2)
- **Snippet**: 2025. 12. 23. — By adopting MLA,DeepSeek-V3achieves a significant reductioninKV cache size, requiring only 70 KB per token, substantially less thanLLaMA- ...
- **Engine**: searxng
### 20. [(PDF)Comparativeanalysisoftheperformanceofthe large language ...](https://www.researchgate.net/publication/393476014_Comparative_analysis_of_the_performance_of_the_large_language_models_DeepSeek-V3_DeepSeek-R1_open_AI-O3_mini_and_open_AI-O3_mini_high_in_urology)
- **Snippet**: 2025. 7. 7. — DeepSeek‑V3 achieved solid baseline correctness but made fewer successful corrections on subsequent attempts. Although OpenAI o3‑mini initially ...
- **Engine**: searxng
### 21. [DeepSeek-V3vsLlama3.18B Instruct](https://llm-stats.com/models/compare/deepseek-v3-vs-llama-3.1-8b-instruct)
- **Snippet**: 2024. 12. 25. —DeepSeek-V3outperformsin5 benchmarks (DROP, GPQA, IFEval, MMLU, MMLU-Pro), whileLlama3.18B Instruct is better at 0 benchmarks.
- **Engine**: searxng
### 22. [DeepSeek-V3: EfficientandScalable AI with Mixture-of-Experts](https://medium.com/aimonks/deepseek-v3-efficient-and-scalable-ai-with-mixture-of-experts-8bd945b5ea3f)
- **Snippet**: Despite the high test accuracy, low time complexity,andsatisfactoryperformanceofDeepSeek-V3, this study has several shortcomings. Its large ...
- **Engine**: searxng
### 23. [The Complete Guide to DeepSeek Models: V3, R1, V3.1, V3.2andBeyond](https://www.bentoml.com/blog/the-complete-guide-to-deepseek-models-from-v3-to-r1-and-beyond)
- **Snippet**: 2025. 12. 1. — Understand the differences betweenDeepSeek-V3, R1, V3.1, V3.2,anddistilled models. Learn how to choose the right modelanddeploy them ...
- **Engine**: searxng
### 24. [BenchmarkevaluationofDeepSeek large language modelsinclinical ...](https://www.nature.com/articles/s41591-025-03727-2.pdf)
- **Snippet**: S Sandmann 저술 · 2025 · 112회 인용 — Here, to demonstrate the clinical utilityofDeepSeek-V3andDeepSeek-R1, we benchmarked theirperformanceon clinical decision support tasks ...
- **Engine**: searxng
### 25. [I compared DeepSeek R1 with MetaLlama3.1-405b for text ...](https://www.facebook.com/groups/DeepNetGroup/posts/2422473858145482/)
- **Snippet**: I compared DeepSeek R1 with MetaLlama3.1-405b for text summarizationandclassification. SurprisinglyLlamaperformed better. I present the resultsinthis ...
- **Engine**: searxng
### 26. [DeepSeek-V3: The PinnacleofOpen-Source AI - Swapan Rajdev](https://www.srajdev.com/p/deepseek-v3-the-pinnacle-of-open-source-ai)
- **Snippet**: 2025. 1. 3. —DeepSeek-V3currently stands as the best open-source AI model, decisively outperforming competitors such asLlama3.1405B, Qwen,andMistralinbenchmarks.
- **Engine**: searxng

## 3. Reading Layer (Content Extraction)
The Reader visited the URLs and extracted full text:
- **[1] https://arxiv.org/html/2412.19437v1**
  - Status: Success
  - Extracted Characters: 120091
- **[2] https://www.techrxiv.org/users/962529/articles/1331367-unpacking-deepseek-v3-from-architectural-renovations-to-technical-innovations**
  - Status: Failed (Failed to download: Status 403)
  - Extracted Characters: 0
- **[3] https://sonovman.blogspot.com/2025/02/deepseek-model-v3-r1.html**
  - Status: Success
  - Extracted Characters: 52932
- **[4] https://dl.acm.org/doi/10.1145/3695053.3731412**
  - Status: Failed (Failed to download: Status 403)
  - Extracted Characters: 0
- **[5] https://arxiv.org/pdf/2412.19437**
  - Status: Failed (pypdf not installed)
  - Extracted Characters: 0

## 4. Refinement Layer (RAG Evidence)
The Re-ranker selected the following chunks as 'High Quality Evidence' for the LLM:
### Chunk 1 (Score: 0.90)
> From a more detailed perspective, we compare DeepSeek-V3-Base with the other open-source base models individually.
(1) Compared with DeepSeek-V2-Base, due to the improvements in our model architecture, the scale-up of the model size and training tokens, and the enhancement of data quality, DeepSeek-...

*Source: https://arxiv.org/html/2412.19437v1*
### Chunk 2 (Score: 0.81)
> Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware. During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 d...

*Source: https://arxiv.org/html/2412.19437v1*
### Chunk 3 (Score: 0.78)
> The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017) framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with DeepSeek-V2, an exception is that w...

*Source: https://arxiv.org/html/2412.19437v1*
### Chunk 4 (Score: 0.45)
> DeepSeek-R1: Incentivizing ReasoningCapability in LLMs via ReinforcementLearning
Microsoft의 통합 AI 플랫폼인 Azure AI Foundry 팀은 발 빠르게 Azure AI Foundry 및 Github와 시맨틱(Semantic) 커널에서 DeepSeek-R1을 사용할 수 있다고 발표했습니다.
본 고에서는 각 기술보고서와 논문에서 제시한 모델, 아키텍처와 접근방법을 좀더 상세하게 설명하고자 합니다.
## 2. DeepSeek-V3
671 억 개의 총 매개 변수...

*Source: https://sonovman.blogspot.com/2025/02/deepseek-model-v3-r1.html*
### Chunk 5 (Score: 0.29)
> *<참고자료>*
*- DeepSeek-AI, Liu, A., Feng, B., Xue, B.,
Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D.,
Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., … Pan, Z. (2024). DeepSeek-V3 Technical Report (No.
arXiv:2412.19437). arXiv. https://doi.org/10.48550/arXiv...

*Source: https://sonovman.blogspot.com/2025/02/deepseek-model-v3-r1.html*

## 5. Cognitive Layer (Final Answer)
The LLM synthesized this answer based **ONLY** on the evidence above:

```
DeepSeek-V3 incorporates architectural innovations such as Multi-Head Latent Attention (MHLA) and DeepSeekMoE (a variant of Model Mixing of Experts, or MoE), which are designed to improve both training efficiency and inference performance [3]. These components are built upon the foundational Transformer architecture (Vaswani et al., 2017) and have been previously validated in DeepSeek-V2 [3]. Specifically, DeepSeek-V3 introduces an auxiliary-loss-free load balancing strategy to mitigate performance degradation typically caused by load balancing efforts in MoE systems [3]. This refinement enhances the stability and effectiveness of the MoE mechanism during training and inference.

In terms of training cost efficiency, DeepSeek-V3 demonstrates significant advantages over LLaMA-3.1 405B Base. According to the provided context, DeepSeek-V3-Base achieves superior performance on multilingual, code, and math benchmarks despite having only half the activated parameters of Qwen2.5 72B and significantly fewer activated parameters than LLaMA-3.1 405B (which has 11 times more activated parameters) [1]. More importantly, DeepSeek-V3 is engineered with an optimized co-design of algorithms, frameworks, and hardware, resulting in substantially lower training costs.

The pre-training stage of DeepSeek-V3 requires only 180,000 H800 GPU hours (equivalent to 3.7 days on a cluster of 2048 H800 GPUs), completing in under two months and costing a total of 2.788 million GPU hours for full training [2]. At a rental rate of $2 per GPU hour, this amounts to a total training cost of $5.576 million — a highly efficient figure considering the scale of the model [2]. This cost efficiency is attributed to the architectural and training optimizations, including MLA and DeepSeekMoE, which reduce computational overhead during both training and inference.

In summary, DeepSeek-V3 leverages MHLA and DeepSeekMoE to improve performance and efficiency, and its training cost is notably lower than that of LLaMA-3.1 405B, even though it operates with fewer activated parameters [1][2][3].

**References:**  
[1] https://arxiv.org/html/2412.19437v1  
[2] https://arxiv.org/html/2412.19437v1  
[3] https://arxiv.org/html/2412.19437v1  
[4] https://sonovman.blogspot.com/2025/02/deepseek-model-v3-r1.html  
[5] https://sonovman.blogspot.com/2025/02/deepseek-model-v3-r1.html
```
